---
title: "Introduction to Bayesian Analysis"
author: "Stephen Rhodes - srhodes@research.baycrest.org"
date: 'Last updated: `r Sys.Date()`'
output:
  beamer_presentation: default
  ioslides_presentation:
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## Overview

- Introduction to Bayesian analysis
    - What's the purpose of Bayesian estimation?
    - MCMC sampling (Metropolis algorithm and Gibbs)
    - How good are my samples?
    - Posterior predictive checks
    - Model comparison
    
# Introduction to Bayesian estimation

## Purpose

> - To re-allocate credibility over parameters values based on the observed data (Kruschke, 2015)
> - Given the observed data, what parameter values should we most strongly believe in?
> - To obtain this we need to start with a model and some *prior* expectation as to the probability of certain parameter values in this model (more on this later)

## Bayesian estimation

- $\theta$ = parameter(s), $y$ = observed data
- $p(y \mid \theta)$ = the likelihood
- $p(\theta)$ = the prior
- $p(y)$ = probability of the data ("the evidence")
<!-- 
- The model: $p(\theta, y) = p(\theta)p(y \mid \theta)$ (see Gelman et al., 2013)
- To allocate credibility to the parameter values we can "condition" on the observed data. This gives us the *posterior* distribution of the parameters given the data:
-->

$$
p(\theta \mid y) = \frac{p(\theta)p(y \mid \theta)}{p(y)}
$$

## Bayesian estimation

$p(y)$ does not depend on the model parameters so we can omit it in favor of the *unnormalized posterior*

$$
p(\theta \mid y) \propto p(\theta)p(y \mid \theta)
$$

**The posterior is proportional to the prior times the likelihood**

## Conceptual example

Linear regression:

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$

$$
\epsilon_i \sim \mbox{Normal}(0, \sigma)
$$

Or

$$
y_i \sim \mbox{Normal}(\beta_0 + \beta_1x_i, \sigma)
$$

## Conceptual example

We need a prior, $p(\beta_0, \beta_1, \sigma)$

If $x$ and $y$ have been scaled ($z$-scored), a reasonable choice would be:

- $\beta_0 \sim \mbox{Normal}(0, 3)$
- $\beta_1 \sim \mbox{Normal}(0, 3)$
- $\sigma \sim \mbox{Cauchy}(0, 2.5)^{+}$

## Conceptual example

These priors essentially say that we expect either a positive or negative relationship between $x$ and $y$. 

If we had strong reason to expect that $y$ should increase with $x$ we could instead use:

- $\beta_1 \sim \mbox{Normal}(0, 3)^{+}$


# MCMC sampling

## Why sample?

> - $p(\theta \mid y)$ is a *distribution* but the shape of that distribution is not always directly attainable.
> - For a normal prior on the mean and a normal likelihood function the posterior distribution for the mean is also a normal distribution. 
> - The normal is a *conjugate prior* for the normal likelihood. (The Beta distribution is conjugate for the binomial distribution).
> - For most models an analytic expression of the posterior distribution is not possible.
> - In these situations sampling is needed to approximate the posterior distribution. This is what is offered by software like JAGS, BUGS, Stan etc.

## Intro to MCMC

Markov Chain Monte Carlo

- *Markov Chain* - a "memoryless" chain of events. Each step depends on the current state (e.g. parameter value) and not previous ones
- *Monte Carlo* - Repeated random sampling

Goal of MCMC - to approximate a target distribution

## The Metropolis Algorithm

- We want to approximate $p(\theta \mid y)$ which is proportional to the prior times the likelihood. 
- The Metropolis algorithm allows us to explore the space of possible parameter values. Values are "visited" in proportion to their density under the posterior.

## The Metropolis Algorithm

> 1. Select a stating point in the parameter space. This is $\theta_{\text{current}}$
> 2. A move in parameter space is suggested from a *proposal distribution*. This is $\theta_{\text{new}} = \theta_{\text{current}} + \Delta\theta$
> 3. The probability that we accept the proposed move is: $p_{\text{accept}} = \min\left(\frac{f(\theta_{\text{new}})}{f(\theta_{\text{current}})}, 1\right)$
> 4. Sample a uniform number between 0 and 1. If this number is smaller than $p_{\text{accept}}$ then we accept the proposal and $\theta_{\text{new}}$ becomes $\theta_{\text{current}}$ for the next step. If not we stick with the $\theta_{\text{current}}$ we had originally.
> 5. Repeat steps 2-4 many times and record $\theta_{\text{current}}$ at the beginning of each loop.

Note for our Bayesian purposes $f(x) = p(x)p(y \mid x)$.

## The Metropolis Algorithm

Why does it work?

> - If the proposed parameter value is "better" then the current value (i.e. produces a higher posterior density) then it is *always* visited.
> - If it is not better than the current value it is visited in proportion to how it compares to the current value: $f(\theta_{\text{new}})/f(\theta_{\text{current}})$
> - Thus, values with a higher posterior density are visited more often. Running a long chain and assessing how often certain parameter values are represented gives us the posterior even if we don't know how to express its form analytically.

## Metropolis Example

```{r, echo=FALSE, fig.show='hide'}

# load the functions
source("metropolis-example.R")

library(coda)

```

- Suppose we want to estimate the mean ($\mu$) IQ (or some other cognitive) score of a group of participants (N = `r N`).
- The test is scaled to have a population standard deviation of 15 so we assume that this is known.

```{r, echo=FALSE, fig.height=4}

hist(Y, xlab = "Score", main = "Test Scores", col = "seagreen3")

```

## Metropolis Example

- We need to choose a prior distribution for $\mu$
- Here we use a normal distribution with a mean of `r prior_m` and standard deviation of `r prior_sd`

```{r, echo=FALSE}

curve(dnorm(x = x, prior_m, prior_sd), from = prior_m-prior_sd*3, to = prior_m+prior_sd*3, xlab = bquote(mu), ylab="", main = "Prior", col = "darkgreen", lwd = 2)

```

## Metropolis Example

- The script `metropolis-example.R` contains code implementing a Metropolis algorithm

```{r, echo=FALSE}

# plot the steps in the chain
plot(samples, xlab = "Step", ylab = bquote(mu), type="l", col="blue")

```

## Let's take a look at some steps

<div class="columns-2">

```{r, echo=FALSE, fig.width=5}

step = 500:504

mu = c(105, 111, 111, 109, 113)

plot(step, mu, xlab = "Step", ylab = bquote(mu), type="l", col="blue")
points(step[1], mu[1], pch=16, cex=2, col="red")

```

</br>

**Step 500**

- The current value of $\mu$ is `r mu[1]`. A new value is proposed from a normal distribution with a mean of `r mu[1]` and a standard deviation of 5. The proposed value is 111.
- Next we calculate $p(\mu)p(y \mid \mu)$ for both the current and proposed values of $\mu$ (we use the `dnorm` function twice as both the prior and the likelihood are normal).
- In this case $p_{\text{accept}} = 1$ so we accept the proposal for step 501

</div>

## Let's take a look at some steps

<div class="columns-2">

```{r, echo=FALSE, fig.width=5}

plot(step, mu, xlab = "Step", ylab = bquote(mu), type="l", col="blue")
points(step[2], mu[2], pch=16, cex=2, col="red")

```

</br>

**Step 501**

- The current value of $\mu$ is `r mu[2]`. The proposed value is 103.
- The ratio of $p(\mu)p(y \mid \mu)$ for proposed relative to current value of $\mu$ gives $p_{\text{accept}} = .4$
- The number sampled via `runif(n = 1, min = 0, max = 1)` is 0.6 so the proposal is *rejected*
- We stick with 111 for the next step

</div>

## Let's take a look at some steps

<div class="columns-2">

```{r, echo=FALSE, fig.width=5}

plot(step, mu, xlab = "Step", ylab = bquote(mu), type="l", col="blue")
points(step[3], mu[3], pch=16, cex=2, col="red")

```

</br>

**Step 502**

- The current value of $\mu$ is `r mu[3]`. The proposed value is `r mu[4]`.
- The ratio of $p(\mu)p(y \mid \mu)$ for proposed relative to current value of $\mu$ gives $p_{\text{accept}} = .8$
- The number sampled via `runif(n = 1, min = 0, max = 1)` is 0.34 so the proposal is *accepted*
- The current value is set to `r mu[4]` for step 503.

and so on...

</div>

## It works! 

For a normal likelihood with a normal prior on the mean the form of the posterior distribution is known. So we can compare the results. 

```{r, echo=FALSE}

# plot the histogram
hist(samples, xlab = bquote(mu), main = paste0("N samples = ", length(samples)), col = "grey", breaks = 30, probability = T)

# calculate the exact form of the posterior to compare to the samples
#post = posterior_quantities(y = Y, prior_mean = prior_m, prior_sd = prior_sd)
curve(dnorm(x, mean = post[1], sd = post[2]), from = min(samples), to = max(samples), col = "red", lwd = 2, add = T)

```

## It works! 

Increasing the number of steps improves the approximation.

```{r, echo=FALSE}

samples2 = metropolis_iq(y = Y, 
                        prior_mean = prior_m,
                        prior_sd = prior_sd, 
                        proposal_sd = 5,
                        n_samples = 10000, 
                        start = 100)

# plot the histogram
hist(samples2, xlab = bquote(mu), main = paste0("N samples = ", length(samples2)), col = "grey", breaks = 30, probability = T)

# calculate the exact form of the posterior to compare to the samples
#post = posterior_quantities(y = Y, prior_mean = prior_m, prior_sd = prior_sd)
curve(dnorm(x, mean = post[1], sd = post[2]), from = min(samples2), to = max(samples2), col = "red", lwd = 2, add = T)

```

# How accurate is the MCMC chain?

## Things to consider

- Burn in (or warm up)
- Autocorrelation
- Effective Sample Size (ESS)
- Convergence and the Potential Scale Reduction Factor (PSRF)
- Thinning

## Burn in (or warm up)

```{r, echo=FALSE}

samples3 = metropolis_iq(y = Y, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = 1, n_samples = 1000, start = 0)

plot(samples3, xlab = "Step", ylab = bquote(mu), type="l", col="blue")

```

## Autocorrelation

```{r, echo=FALSE}

samples4 = metropolis_iq(y = Y, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = .1, n_samples = 1000, start = 110)

plot(samples4, xlab = "Step", ylab = bquote(mu), type="l", col="blue")

```

## Autocorrelation

```{r, echo=FALSE}

par(mfrow=c(1,2))
autocorr.plot(as.mcmc(samples4), auto.layout = F)
autocorr.plot(as.mcmc(samples), auto.layout = F)

```

## Effective Sample Size

A way of estimating the number of independent samples once accounting for autocorrelation:

$$
ESS = \frac{N}{1 + 2\sum_{k = 1}^{\infty} \rho_k}
$$

Where $\rho_k$ is the autocorrelation at lag $k$. Think of this as dividing the number of samples by the amount of autocorrelation. In practice the sum stops when the autocorrelation is small (e.g. $\rho_k < 0.05$; see Kruschke, 2015, p. 184).

## Convergence

How do we know that we have converged onto a stable distribution?

```{r, echo=FALSE}

samples5 = metropolis_iq(y = Y, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = 5, n_samples = 1000, start = 100)

samples6 = metropolis_iq(y = Y, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = 5, n_samples = 1000, start = 120)

plot(samples5, xlab = "Step", ylab = bquote(mu), type="l", col="blue", ylim=c(100, 120))
lines(samples6, col = "orchid")
abline(v = 100, col="red", lty=2)

```

## Convergence

> - Run multiple sequences (*chains*) with different starting points
> - Compare variation between different chains to variation within chains. When these are approximately equal we can claim to have converged on the target distribution.
> - This is measured via $\hat{R}$ and conventionally a value $< 1.1$ is considered converged.
> - $\hat{R}$ is also referred to as the Gelman-Rubin convergence diagnostic or the Potential Scale Reduction Factor (PSRF).

## Convergence

```{r, echo=FALSE, fig.width=10}

par(mfrow=c(1,2))
# converged
plot(samples5, xlab = "Step", ylab = bquote(mu), type="l", col="blue", ylim=c(100, 120))
lines(samples6, col = "orchid")

R_hat = round(gelman.diag(mcmc.list(as.mcmc(samples5), as.mcmc(samples6)))$psrf[1,1], 2)

text(x = 600, y = 118, labels = bquote(hat(R) %~~% .(R_hat)))

# not converged
samples7 = metropolis_iq(y = Y - 5, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = 5, n_samples = 1000, start = 118)

plot(samples5, xlab = "Step", ylab = bquote(mu), type="l", col="blue", ylim=c(100, 120))
lines(samples7, col = "orchid")

R_hat = round(gelman.diag(mcmc.list(as.mcmc(samples5), as.mcmc(samples7)))$psrf[1,1], 2)

text(x = 600, y = 118, labels = bquote(hat(R) %~~% .(R_hat)))

```

## Thinning

```{r, echo=FALSE, fig.width=10}

par(mfrow=c(1,2))

samples8 = metropolis_iq(y = Y, prior_mean = prior_m, prior_sd = prior_sd, proposal_sd = .5, n_samples = 10000, start = 110)

plot(samples8, xlab = "Step", ylab = bquote(mu), main = "Full Chain", type="l", col="blue")

plot(samples8[seq(1, 10000, 10)], xlab = "Step", ylab = bquote(mu), main = "Keep every 10th sample", type="l", col="blue")

```

## Thinning

```{r, echo=FALSE, fig.width=10}

par(mfrow=c(1,2))

autocorr.plot(as.mcmc(samples8), auto.layout = F, main = "Full Chain")
autocorr.plot(as.mcmc(samples8[seq(1, 10000, 100)]), auto.layout = F, main = "Keep every 10th sample")

```

## Thinning

- If autocorrelation is really bad thinning might help, but it might suggest deeper problems with your model (see Gelman et al., 2014)
- It has been claimed that thinning is "often unnecessary and always inefficient" ([Link & Eaton, 2012](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210X.2011.00131.x)). 
- Often it is better to keep the full chains than throw information away.

# Gibbs Sampling

## Reminder of the Metropolis algorithm

1. Select a stating point in the parameter space.
2. A move in parameter space is suggested from a *proposal distribution*.
3. We work out the probability of accepting the move via $p_{\text{accept}} = \min\left(\frac{f(\theta_{\text{new}})}{f(\theta_{\text{current}})}, 1\right)$.
4. If a uniform(0, 1) number is smaller than $p_{\text{accept}}$ then we accept the new parameter value for the next cycle.
5. Repeat steps 2-4 many times.

## Gibbs Sampling

More complicated than our Metropolis algorithm but better for large numbers of parameters, which easily occurs in hierarchical modeling. Imagine a two parameter situation $\{\theta_1, \theta_2\}$ with independent prior distributions $p(\theta_1, \theta_2) = p(\theta_1)p(\theta_2)$.

> 1. Start at a random point in the (two dimensional) parameter space.
> 2. Select one of the parameters (e.g. $\theta_1$). Sample a new value for it from $p(\theta_1 \mid \theta_2, y)$ using the current value of the other parameters (in this case, $\theta_2$).
> 3. The sampled value comes directly from the posterior of $\theta_1$ conditional on the current value of $\theta_2$ so the move is *always* accepted.
> 4. Go back to step 2 and repeat many times.

To do this we need to know the conditional distributions for each parameter given each of the others $p(\theta_i \mid \theta_{i \neq j}, y)$. Luckily software (Stan) can do this for us!

# Posterior predictive checks

## Posterior predictive checks

PPCs are a way of evaluating the performance of a particular model and identifying potential areas/ sources of misfit.

# Model comparison

## Model comparison

- Bayes' factors (based on marginal likelihood, or posterior density)
- Log predictive density (loo, WAIC)

## End of introduction to Bayesian analysis

This drew heavily on:

- Kruschke (2015). Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Chapter 7.
- Gelman et al. (2014). Bayesian Data Analysis (3rd edition). Chapters 11 and 12.

